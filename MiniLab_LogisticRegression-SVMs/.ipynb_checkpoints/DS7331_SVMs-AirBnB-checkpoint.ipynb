{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andy Nguyen, Michael Wolfe, Spencer Fogelman, & Joseph Caguioa\n",
    "\n",
    "DS 7331.407\n",
    "\n",
    "Thursday 6:30pm - 8:00pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression and SVMs of AirBnB Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "[50 Points]\n",
    "* Assess performance of each model using 80/20 training-test split\n",
    "* Adjust model parameters to optimize accuracy\n",
    "    * if dataset size requires stochastic gradient descent, then only linear kernel is appropriate\n",
    "    \n",
    "[10 Points]\n",
    "* Discuss advantages of each model for each classification task\n",
    "* Does one type of model offer superior performance over another in terms of prediction accuracy?\n",
    "    * In terms of training time or efficiency? Explain.\n",
    "\n",
    "[30 points]\n",
    "* Use weights from logistic regression to interpret importance of different features for each classification task. Explain interpretation in detail.\n",
    "    * Why do you think some variables are more important?\n",
    "    \n",
    "[10 points]\n",
    "* Look at the chosen support vectors for the classifcation task. Do these provide any insight into the data? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Dataset Source: https://www.kaggle.com/rudymizrahi/airbnb-listings-in-major-us-cities-deloitte-ml*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Kaggle dataset contains data on Airbnb listings for six major U.S. cities. The competition's original goal was to use available attributes to predict listing price. However, the inherent richness of the variables also allows\n",
    "\n",
    "Airbnb Superhosts are experienced property owners who guests have helped rate as the best of the best. Earning this designation requires obtaining a minimum number of yearly stays, maintaining a high response rate, keeping above a certain score average, and avoiding cancellations. However, is it possible to predict Superhost status off of other attributes on the listing? For simplicity we define a Superhost as someone with a score average greater than or equal to 96%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "#import plotly.graph_objects as go\n",
    "import datetime\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/anguyen-07/DS7331-ML_Labs/master/data/airbnb_train.csv')\n",
    "df['grade'] = pd.cut(df.review_scores_rating, [0,60,70,80,90,101], right=False, labels = ['F', 'D', 'C', 'B', 'A'])\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to preparing the Airbnb dataset for use with logistic regression and support vector machines is to remove or impute missing data and change the variables to compatible datatypes. Much of the logic behind this work was covered in Lab 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleanup (from first project)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Clean up datatypes and duplicates\n",
    "df_ratings = df.dropna(subset=['review_scores_rating'])\n",
    "floats = ['log_price','bathrooms','latitude','longitude','review_scores_rating']\n",
    "df_ratings[floats] = df_ratings[floats].astype(np.float64)\n",
    "ints = ['id','accommodates','number_of_reviews','bedrooms','beds']\n",
    "df_ratings[\"host_response_rate\"] = df_ratings[\"host_response_rate\"].str.rstrip('%').astype(np.float64)/100\n",
    "date_time = ['first_review','host_since','last_review']\n",
    "df_ratings[date_time] = df_ratings[date_time].apply(pd.to_datetime)\n",
    "booleans = ['host_has_profile_pic','host_identity_verified','instant_bookable']\n",
    "df_ratings[booleans] = df_ratings[booleans].replace({'t':True,'f':False})\n",
    "df_ratings[booleans] = df_ratings[booleans].astype(np.bool)\n",
    "categorical = ['property_type','room_type','bed_type','cancellation_policy','city','neighbourhood','zipcode']\n",
    "df_ratings[categorical] = df_ratings[categorical].astype('category')\n",
    "df_ratings.drop_duplicates()\n",
    "df_ratings.host_since[df_ratings.host_since.isna()] = df_ratings.first_review[df_ratings.host_since.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Impute missing values\n",
    "df_imputed = df_ratings\n",
    "df_imputed[\"bathrooms\"] = df_imputed[\"bathrooms\"].fillna(df_imputed.groupby([\"property_type\",\"accommodates\"])[\"bathrooms\"].apply(lambda x : x.fillna(x.median())))\n",
    "df_imputed[\"bedrooms\"] = df_imputed[\"bedrooms\"].fillna(df_imputed.groupby([\"property_type\",\"accommodates\"])[\"bedrooms\"].apply(lambda x : x.fillna(x.median())))\n",
    "df_imputed[\"beds\"] = df_imputed[\"beds\"].fillna(df_imputed.groupby([\"property_type\",\"accommodates\"])[\"beds\"].apply(lambda x : x.fillna(x.median())))\n",
    "df_imputed[\"host_response_rate\"] = df_imputed[\"host_response_rate\"].fillna(df_imputed.groupby([\"number_of_reviews\"])[\"host_response_rate\"].apply(lambda x : x.fillna(x.mean())))\n",
    "# Impute Missing Value of for 100% Host Response Rate for Row 48194 - Private Room in Apartment\n",
    "df_imputed[\"host_response_rate\"][df_imputed[\"host_response_rate\"].isna()] = 1.0\n",
    "df_imputed[ints] = df_imputed[ints].astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, some of the existing attributes can be transformed into a potentially more useful datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed['price'] = np.exp(df_imputed['log_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#Create a new cleaned amenities column where all amenities are in list form\n",
    "df_imputed['amenities_new'] = df_imputed.apply(lambda row: re.sub(r'[{}\"\"]', '', row['amenities']), axis=1)\n",
    "df_imputed['amenities_new'] = df_imputed.apply(lambda row: row['amenities_new'].lower().split(','), axis=1)\n",
    "df_imputed = df_imputed.reset_index()\n",
    "df_imputed['length_amenities'] = df_imputed.apply(lambda row: len(row['amenities_new']), axis=1)\n",
    "\n",
    "# Create separate columns based on amenities\n",
    "df_imputed['internet'] = df_imputed.apply(lambda row: 'internet' in row.amenities.lower(), axis=1)\n",
    "df_imputed['TV'] = df_imputed.apply(lambda row: 'tv' in row.amenities.lower(), axis=1)\n",
    "df_imputed['air_conditioning'] = df_imputed.apply(lambda row: 'air conditioning' in row.amenities.lower(), axis=1)\n",
    "df_imputed['kitchen'] = df_imputed.apply(lambda row: 'kitchen' in row.amenities.lower(), axis=1)\n",
    "df_imputed['pool'] = df_imputed.apply(lambda row: 'pool' in row.amenities.lower(), axis=1)\n",
    "df_imputed['parking'] = df_imputed.apply(lambda row: 'parking' in row.amenities.lower(), axis=1)\n",
    "\n",
    "# Get information from description based on length in characters\n",
    "df_imputed['description_length'] = df_imputed['description'].apply(len)\n",
    "\n",
    "# Create the target variable superuser\n",
    "df_imputed['superuser'] = False\n",
    "df_imputed.loc[df.review_scores_rating >=96, 'superuser'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "date_published = datetime.datetime(2018,3,14)\n",
    "df_imputed['host_since'] = pd.to_datetime(df_imputed['host_since'])\n",
    "df_imputed['host_since_days'] = df_imputed.apply(lambda row: (date_published - row['host_since']).days, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns that are not meaningful in a modeling context (e.g., index, id), redundant with newly engineered features (e.g., amenities, description), or likely not useful in predicting the target variable superuser (e.g., zipcode, thumbnail_url) are removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete useless columns\n",
    "df_logistic = df_imputed.drop(['index', 'id', 'log_price', 'amenities', 'description', 'zipcode', 'description',\n",
    "                      'latitude', 'longitude', 'name', 'neighbourhood', 'review_scores_rating',\n",
    "                      'thumbnail_url', 'zipcode'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_vars = ['property_type', 'room_type', 'bed_type', 'cancellation_policy', 'city']\n",
    "df_logistic[categorical_vars].apply(lambda x: len(x.value_counts()), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove factor with unnecessarily large number of levels\n",
    "df_logistic.drop(['property_type'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_logistic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=df_logistic, x='superuser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple bar chart of the counts shows that both classes of Superuser are well-represented in the remaining data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = ['host_since_days', 'description_length', 'length_amenities', 'price', 'beds',\n",
    "                     'bedrooms', 'number_of_reviews', 'bathrooms', 'accommodates', 'host_response_rate']\n",
    "sns.heatmap(df_logistic[numerical_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collinearity does not appear to be a concern based on the numeric variables. However, it is worth noting that there are vast scaling differences. For example, host_since_days counts the number of days a listing existed until a given date, resulting in a mean in the thousands because many listings have been active for several years. In contrast, host_response_rate is a percentage. Standardizing these values to account for this may be useful, and is explored later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OneHotEncoding - Create Dummy Variables for Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables\n",
    "multiCategorical_vars = ['city', 'cancellation_policy','bed_type', 'room_type']\n",
    "\n",
    "dummy_df = pd.get_dummies(df_logistic[multiCategorical_vars], drop_first=False)\n",
    "\n",
    "df_logistic_dummy = pd.concat([df_logistic, dummy_df], axis=1)\n",
    "\n",
    "# Delete multi-categorical variables\n",
    "for x in multiCategorical_vars:\n",
    "    if x in df_logistic_dummy:\n",
    "        del df_logistic_dummy[x]\n",
    "\n",
    "# Delete other redundant variables\n",
    "del df_logistic_dummy['amenities_new']\n",
    "del df_logistic_dummy['host_since']\n",
    "del df_logistic_dummy['grade']\n",
    "del df_logistic_dummy['first_review']\n",
    "del df_logistic_dummy['last_review']\n",
    "\n",
    "df_logistic_dummy.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Test Set Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "# Use X to predict the binary class of y\n",
    "## X and Y are numpy matricies - .values call converts the data type to simple matrices\n",
    "if 'superuser' in df_logistic_dummy:\n",
    "    y = df_logistic_dummy['superuser'].values # retrieve binary value for class distinction\n",
    "    del df_logistic_dummy['superuser'] # get rid of True/False label\n",
    "    X = df_logistic_dummy.values # use all other attribute values to predict\n",
    "    \n",
    "num_cv_iterations = 10\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n_splits = num_cv_iterations,\n",
    "                        test_size = 0.2,\n",
    "                        random_state = 123) # set random number seed\n",
    "cv_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run through the cross-validation loop object previously set-up\n",
    "## Training and Test data should be set for one single interation\n",
    "for train_indicies, test_indicies in cv_object.split(X,y):\n",
    "    X_train = X[train_indicies]\n",
    "    y_train = y[train_indicies]\n",
    "    \n",
    "    X_test = X[test_indicies]\n",
    "    y_test = y[test_indicies]\n",
    "    \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# we want to normalize the features based upon the mean and standard deviation of each column. \n",
    "# However, we do not want to accidentally use the testing data to find out the mean and std (this would be snooping)\n",
    "# to Make things easier, let's start by just using whatever was last stored in the variables:\n",
    "##    X_train , y_train , X_test, y_test (they were set in a for loop above)\n",
    "\n",
    "# scale attributes by the training set\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train) # find scalings for each column that make this zero mean and unit std\n",
    "# the line of code above only looks at training data to get mean and std and we can use it \n",
    "# to transform new feature data\n",
    "\n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test) # apply those means and std to the test set (without snooping at the test set values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# Create SVM training object\n",
    "svm_clf = SVC(C = 0.5, kernel = 'rbf', degree = 3, gamma = 'auto')\n",
    "# Train the SVM model\n",
    "svm_clf.fit(X_train_scaled, y_train)\n",
    "# Predict based on trained model - y_hat = model predictions\n",
    "y_hat = svm_clf.predict(X_test_scaled)\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('SVM Model Accuracy (Linear Kernel)', acc)\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## IGNORE - work in progress\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "Xcont = df_logistic[numerical_features].values\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit(Xcont).transform(Xcont) # fit data and then transform it\n",
    "\n",
    "lda = LDA(n_components=2)\n",
    "X_lda = lda.fit(df_logistic[numerical_features], y).transform(df_logistic[numerical_features]) # fit data and then transform it\n",
    "\n",
    "# print the components\n",
    "print ('pca:', pca.components_)\n",
    "print ('lda:', lda.scalings_.T)\n",
    "\n",
    "plt.scatter(df_logistic['cleaning_fee'],df_logistic['room_type'], c = y, cmap = 'winter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of Support Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>[10 pts] Look at the chosen support vectors for the classification task. Do these provide any insight into the data? Explain. If you used stochastic gradient descent (and therefore did not explicitly solve for support vectors), try subsampling your data to train the SVC modelâ€” then analyze the support vectors from the subsampled dataset.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparisons: Advantages, Performance, Efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>[10 Points]\n",
    "* Discuss advantages of each model for each classification task\n",
    "* Does one type of model offer superior performance over another in terms of prediction accuracy?\n",
    "    * In terms of training time or efficiency? Explain.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
